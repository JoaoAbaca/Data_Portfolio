# ğŸ’³ CoinGecko ETL Pipeline

Este proyecto implementa un pipeline de extracciÃ³n, transformaciÃ³n y carga (ETL) utilizando Python y SQLite para procesar informaciÃ³n de criptomonedas desde la API de CoinGecko. El objetivo es construir un flujo robusto y escalable que simule procesos tÃ­picos de un Data Engineer.

---

## ğŸŒŸ Objetivo

Simular un entorno real de procesamiento de datos utilizando prÃ¡cticas profesionales de ingenierÃ­a de datos, incluyendo:

* ExtracciÃ³n de datos desde una API pÃºblica (CoinGecko)
* ValidaciÃ³n y transformaciÃ³n de datos en bruto
* Persistencia en base de datos relacional (SQLite)
* Logging y manejo de errores

---

## ğŸ§± Estructura del proyecto

```
coin_gecko_pipeline/
â”‚
â”œâ”€â”€ extract.py          # Extrae datos desde la API de CoinGecko
â”œâ”€â”€ transform.py        # Limpia y normaliza los datos extraÃ­dos
â”œâ”€â”€ load.py             # Carga los datos transformados en una base SQLite
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ coin_data_raw.json        # Datos crudos extraÃ­dos desde la API
â”‚   â”œâ”€â”€ coin_data_clean.csv       # CSV limpio con los datos procesados
â”‚   â””â”€â”€ coin_data.db              # Base de datos SQLite con la tabla final
â”‚
â””â”€â”€ README.md           # DocumentaciÃ³n del proyecto
```

---

## ğŸ” Datos utilizados

Se utiliza el endpoint `coins/markets` de la API pÃºblica de [CoinGecko](https://www.coingecko.com/en/api/documentation):

* Top 100 criptomonedas por capitalizaciÃ³n de mercado
* Datos clave: nombre, sÃ­mbolo, precio actual, volumen, market cap, cambio porcentual diario, etc.
* Formato original: JSON

---

## âš™ï¸ Requisitos

* Python 3.8+
* Paquetes:

  * `pandas`
  * `requests`
  * `sqlite3` (incluido por defecto)
  * `os`, `json`, `logging`

Instalar dependencias:

```bash
pip install pandas requests
```

---

## ğŸš€ EjecuciÃ³n del pipeline

1. **Extraer los datos**:

```bash
python extract.py
```

2. **Transformar y limpiar**:

```bash
python transform.py
```

3. **Cargar a la base SQLite**:

```bash
python load.py
```

---

## ğŸ§ Lecciones aplicadas

âœ” Buenas prÃ¡cticas de codificaciÃ³n (PEP8, separaciÃ³n de etapas)
âœ” Manejo de errores y validaciones robustas
âœ” Uso de logs para trazabilidad
âœ” NormalizaciÃ³n y limpieza de datos reales
âœ” Persistencia en base de datos relacional

---

## ğŸ“Œ Autor

Joao â€” [Data Developer Portfolio](https://github.com/joaov-dev)
Proyecto 3 de 3 para postulaciÃ³n a Mutt Data (2025)

---
